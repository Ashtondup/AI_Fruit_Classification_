{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "fruit_train = '/content/drive/My Drive/notebook_data_and_output/dataset3/train'\n",
    "fruit_test = '/content/drive/My Drive/notebook_data_and_output/dataset3/test'\n",
    "data_dir = '/content/drive/My Drive/notebook_data_and_output/dataset3'\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transform[x]) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = {x: torch.utils.data.DataLoader(image_datasets[x], shuffle=True, batch_size=32, num_workers=0) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "inputs, classes = next(iter(data_loader['train']))\n",
    "out = utils.make_grid(inputs)\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.max_pool2d(self.relu(self.bn1(self.conv1(x))), 2))\n",
    "        x = self.dropout(F.max_pool2d(self.relu(self.bn2(self.conv2(x))), 2))\n",
    "        x = self.dropout(F.max_pool2d(self.relu(self.bn3(self.conv3(x))), 2))\n",
    "        x = self.dropout(F.max_pool2d(self.relu(self.bn4(self.conv4(x))), 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model if it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/content/drive/My Drive/AshtonColab/model/fruit_classifier.pth'\n",
    "if os.path.exists(model_path):\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    print(\"Loaded saved model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths to save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_file_path = '/content/drive/My Drive/AshtonColab/Metrics/ROC-AUC.txt'\n",
    "f1_score_file_path = '/content/drive/My Drive/AshtonColab/Metrics/F1_Score.txt'\n",
    "accuracy_file_path = '/content/drive/My Drive/AshtonColab/Metrics/Accuracy.txt'\n",
    "loss_file_path = '/content/drive/My Drive/AshtonColab/Metrics/Loss.txt'\n",
    "os.makedirs(os.path.dirname(roc_auc_file_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of EPOCHS\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "EPOCHS = 150\n",
    "patience = 10\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "cross_el = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "best_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training phase\n",
    "    for batch_idx, data in enumerate(data_loader['train']):\n",
    "        x, y = data[0].to(device), data[1].to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()  # Clear gradients from the previous step\n",
    "        output = net(x)  # Forward pass\n",
    "        loss = cross_el(output, y)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Batch {batch_idx + 1}/{len(data_loader['train'])}, Loss: {loss.item()}\")\n",
    "\n",
    "    average_loss = running_loss / len(data_loader['train'])\n",
    "    print(f\"Average Loss for Epoch {epoch + 1}: {average_loss}\")\n",
    "\n",
    "    # Save loss\n",
    "    with open(loss_file_path, 'a') as loss_file:\n",
    "        loss_file.write(f\"Epoch {epoch + 1}: Loss = {average_loss}\\n\")\n",
    "\n",
    "    # Evaluation phase\n",
    "    net.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader['test']:\n",
    "            x, y = data[0].to(device), data[1].to(device)\n",
    "            output = net(x)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_prob.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy for Epoch {epoch + 1}: {acc}\")\n",
    "    with open(accuracy_file_path, 'a') as accuracy_file:\n",
    "        accuracy_file.write(f\"Epoch {epoch + 1}: Accuracy = {acc}\\n\")\n",
    "\n",
    "    # Compute F1 Score (macro-average)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"F1 Score for Epoch {epoch + 1}: {f1}\")\n",
    "    with open(f1_score_file_path, 'a') as f1_file:\n",
    "        f1_file.write(f\"Epoch {epoch + 1}: F1 Score = {f1}\\n\")\n",
    "\n",
    "    # Binarize the labels for multi-class ROC calculation\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "\n",
    "    # Compute ROC curve, TPR, FPR for all classes combined (one-vs-rest approach)\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin.ravel(), np.array(y_prob).ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Save overall TPR, FPR, and AUC\n",
    "    with open(roc_auc_file_path, 'a') as roc_auc_file:\n",
    "        roc_auc_file.write(f\"Epoch {epoch + 1}:\\n\")\n",
    "        roc_auc_file.write(f\"TPR: {tpr}, FPR: {fpr}, AUC: {roc_auc:.2f}\\n\\n\")\n",
    "\n",
    "    # Early stopping and learning rate adjustment\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "        no_improvement_epochs = 0\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "        print(f\"Best model saved after epoch {epoch + 1}\")\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "        print(f\"No improvement for {no_improvement_epochs} epochs\")\n",
    "\n",
    "    scheduler.step(average_loss)\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
